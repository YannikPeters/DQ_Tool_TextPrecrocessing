---
title: "Text preprocessing and data quality"
author: "Yannik Peters"
format: html
editor: visual
bibliography: references.bib
---

# 1. Introduction

The digitalisation has led to an innovation of research objects and research methods. While statistical methods to analyze numerical data have a long tradition, it is especially the automated analysis of text data that has seen huge improvement in recent years. Automated text analysis methods are applied to various data sources, be it social media data, news paper articles, parliamentary speeches, historical texts or literature. In this introduction, we want to focus on an important, necessary and often times challenging aspect related to data quality in text data: the text processing. Text processing can be defined as all changes that are done to the text data after the data collection and before the data analysis. Its main purpose is to bring the raw data in a form that is then suitable for applying specific research methods, but also to reduce the probability of errors. In this sense, text processing is heavily related to the measurement dimension of data quality. On the one hand, text processing can help to reduce measurement errors, by increasing consistency or accuracy. On the other hand, text processing itself can become a source for potential errors. In the TED-On, the "Total Error Framework for Digital Traces of Human Behavior On Online Platforms" [@sen2021] these errors are referred to as `trace reduction errors`. According to Sen et al. an example for this error would be: "Researchers might decide to use a classifier that removes spam, but has a high false positive rate, inadvertently removing non-spam tweets. They might likewise discard tweets without textual content, thereby ignoring relevant statements made through hyper-links or embedded pictures/videos" (p. 413).

In this tutorial we want to go through all the classic steps of text processing and compare as well as recommend different packages and tools to use. The final step is gonna be to create a document-feature-matrix (DFM) or more precise a document-term-matrix (DTM), that is used often times for multiple analysis techniques such as sentiment analysis or topic modeling. For this purpose, we created a small social media data set with posts about the Olympic summer games in Paris 2024. The Olympic summer games can be considered a transnational media event [@hepp2015], which is nowadays of course not only covered by traditional media but is communicatively accompanied on social media. For copyright reasons, we have constructed an artificial data set which does not contain any real content.

# 2. Set up

At first, we will open all relevant libraries. Please make sure to have all relevant packages installed using the `install.packages()` command. We will be using and comparing the `quanteda`, `stringr`, `textclean` and `tm`, basically the some of the most important and widely used text processing and analysis packages in R.

```{r message = FALSE, warning = FALSE}
library(tidyverse)
library(quanteda)
library(textclean)
library(tm)
library(skimr)
```

Finally, we will then load our artificial dataset from the Olympic summer games.

```{r message = FALSE, warning = FALSE}
olympics_data <- read_csv("5.modified_olympics_tweets_RT - Copy.csv")
olympics_data
```

# 3. Application of tools and use case

### 3.1 Basic data (quality) checks

Let's start with checking out the basic structure of our dataset. Here we find 13 variables and 150 observations.

```{r message = FALSE, warning = FALSE}
str(olympics_data)
```

...

```{r message = FALSE, warning=FALSE}
summary(olympics_data)
```

When running the `summary()` function, the overview might get a bit messy depending on the size of the dataframe. We therefore recommend R tools, that offer basic data quality reports and arrange the results clearly like the `skimr` package. Here, you can receive an overview of the various variables of our data set including descriptive statistics and missing values.

```{r warning=FALSE}
skim(olympics_data)
```

### 3.2 Dealing with multiple languages

As the Olympic games are a transnational media event, it does not come as a surprise to receive a multilingual data set. The basic problem about multilingual data sets is that common CSS research methods like topic modeling or sentiment analysis are expected to be in one language. There are different strategies to deal with multilingual data sets. The chosen strategy has to depend on the specific contexts, the applied methods and the research design and questions. We can basically distinguish three or four main strategies with regard to multilingual data sets (see @haukelicht2023, @lind2021a). For this short version, we will discuss three main strategies.

1\) Selecting cases: one language

This approach is basically about choosing cases that only contain documents in one language. For our Twitter/X data set, we could for instance remove all postings which are not English ones.

```{r}

```

Of course, this strategy might result in a representation error as we systematically exclude content from analysis. So let's take a look at the other strategies.

2\) Multiple single language data sets

Another way of dealing with multilingual data sets is to create language specific subsamples of our data. The main advantage of this strategy is, that we do not lose any content due to exclusion or translation errors. However, compared to the other methods there are more validation steps required with regard to the each single language data set (for detailed information see @haukelicht2023, @lind2021a).

```{r}

```

When we check the language datasets, we only find a few documents that are not in English resulting in some very small language sets. Therefore, this strategy might not be the best with regard to our example data.

3\) Translating

The third option of dealing with multilingual datasets is to translate the non-English speaking tweets into English. As this is a just a small, artificial, sample data set, we could actually translate the few tweets manually. In a real case scenario however, analyzing a data set of millions of tweets, you would usually use a automated translation algorithm or method. The most common translation tools are `Google Translator` and `DeepL`. The main advantage of the translation method is to generate one singular data set, which can then be analyzed with one model only. This requires less resources all well. The main disadvantage lies in the potential of translation errors. It is therefore neccesary to evaluate your translation method. For this purpose, let's translate all non-English comments with both tools in order to compare the results.

```{r}

```

### 3.3 Identifying misspelling

Now, we have created one data set with only English language postings. Next, we want to check is whether the text is correct in terms of spelling. Spelling errors are problematic with regard to text data quality as specific words might not be considered as equal to the correct word by specific methods. We therefore want to check the text data for errors. In R, there are many packages for spelling corrections.

### 3.4 Removing stopwords

### 3.5 Removing special characters

### 3.6 Creating a DFM: tokenization and lemmatization

# 4. Discussion

# 5. Literature
